[{"title":"提醒事项","date":"2019-04-18T19:09:46.000Z","path":"2019/04/19/提醒事项/","text":"每日任务 项目 优先级 坚持天数 背单词 1 100天 跑步 2 50天 未竟事业记录表只记录没有完成每日任务的时候","tags":[{"name":"生活点滴","slug":"生活点滴","permalink":"http://yoursite.com/tags/生活点滴/"}]},{"title":"落星","date":"2019-04-18T18:53:28.000Z","path":"2019/04/19/落星/","text":"去不该去的地方，做不该做的事情，这也是勇气啊！ ——《侠肝义胆沈剑心》（2019.04.19）","tags":[{"name":"生活点滴","slug":"生活点滴","permalink":"http://yoursite.com/tags/生活点滴/"}]},{"title":"GitHub设置无密码登录","date":"2019-04-03T12:07:01.000Z","path":"2019/04/03/GitHub设置无密码登录/","text":"GitHub项目的授权方式有两种方式：Https和SSH。 Https可以随意克隆github上的项目，而不管是谁的；而SSH则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。 https url在push的时候是需要验证用户名和密码的；而 SSH在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。 一 安装ssh证书 首先需要检查你电脑是否已经有 SSH key ，在 git Bash 客户端，输入如下代码： 12$ cd ~/.ssh$ ls 1这两个命令就是检查是否已经存在 id_rsa.pub 或 id_dsa.pub 文件，如果文件已经存在，那么则跳过步骤2。 创建一个 SSH key 1$ ssh-keygen -t rsa -C \"your_email@example.com\" 123456代码参数含义：-t 指定密钥类型，默认是 rsa ，可以省略。-C 设置注释文字，比如邮箱。-f 指定密钥文件存储文件名。接着又会提示你输入两次密码（该密码是你push文件的时候要输入的密码，而不是github管理者的密码），可以不输入密码，直接按回车。 添加你的 SSH key 到 github上面去 a. 首先你需要拷贝 id_rsa.pub 文件的内容，你可以用编辑器打开文件复制，也可以用git命令复制该文件的内容，如： 1$ clip &lt; ~/.ssh/id_rsa.pub b. 登录你的github账号，从又上角的settings进入，然后点击菜单栏的 SSH key 进入页面添加 SSH key。 c. 点击 Add SSH key 按钮添加一个 SSH key 。把你复制的 SSH key 代码粘贴到 key 所对应的输入框中，记得 SSH key 代码的前后不要留有空格或者回车。title随意。 测试一下该SSH key 1$ ssh -T git@github.com ​ 当你输入以上代码时，会有一段警告代码，如： 123The authenticity of host 'github.com (207.97.227.239)' can't be established.# RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.# Are you sure you want to continue connecting (yes/no)? ​ 输入 yes 既可。 ​ 如果你创建 SSH key 的时候设置了密码，接下来就会提示你输入密码 ​ 注意：输入密码时如果输错一个字就会不正确，使用删除键是无法更正的。 ​ 密码正确后你会看到下面这段话，如： 12Hi username! You've successfully authenticated, but GitHub does not# provide shell access. ​ 如果用户名是正确的,你已经成功设置SSH密钥。 二 已有的项目切换到使用SSH方式连接​ 安装ssh证书，每次push pull 都需要输入git密码，原因是使用了https方式 push。 在terminal里边 输入 git remote -v ，可以看到形如一下的返回结果： 12origin https://cleey@github.com/cleey/phppoem.git (fetch)origin https://cleey@github.com/cleey/phppoem.git (push) 安装以下方式更换成ssh方式的： 123git remote rm origingit remote add origin git@github.com:cleey/phppoem.gitgit push origin","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"Git 撤销合并操作","date":"2019-01-31T05:01:23.000Z","path":"2019/01/31/Git撤销合并操作/","text":"利用Merge操作合并分支时，可能会出现一些错误，需要撤销合并。这里介绍如何撤销已经上传至github远程仓库的方法 当你使用 git merge 合并两个分支，你将会得到一个commit。执行 git show 之后，会有类似的输出： 123commit 19b7d40d2ebefb4236a8ab630f89e4afca6e9dbeMerge: b0ef24a cca45f9...... 其中，Merge 这一行代表的是合并所用到的两个分支(parents)。举个例子，通常，我们的稳定代码都在 master 分支，而开发过程使用 dev 分支，当开发完成后，再把 dev 分支 merge 进 master 分支： 123a -&gt; b -&gt; c -&gt; f -- g -&gt; h (master) \\ / d -&gt; e (dev) g 是 merge 后得到的代码，g 的两个 parent 分别是 f 和 e。 当你撤销合并，需要添加-m参数来指定撤销合并至哪条分支(parent)。在你合并两个分支并试图撤销时，Git 并不知道你到底需要保留哪一个分支上所做的修改。从 Git 的角度来看，master 分支和 dev 在地位上是完全平等的，只是在 workflow 中，master 被人为约定成了「主分支」。 于是 Git 需要你通过 m 或 mainline 参数来指定「主线」。merge commit 的 parents 一定是在两个不同的线索上，因此可以通过 parent 来表示「主线」。m 参数的值可以是 1 或者 2，对应着 parent 在 merge commit 信息中的顺序。因而，撤销g的合并操作恢复至原主分支f上：12# g为merge后的索引号git revert -m 1 g 从而变成：123a -&gt; b -&gt; c -&gt; f -- g -&gt; h -&gt; G -&gt; i (master) \\ / d -&gt; e -&gt; j -&gt; k (dev) 此外，由于撤销操作，则在下一次dev与master合并时，merge操作不会合并d、e两个版本代码。因为git认为已经合并或没有合并的需要。此刻，由于新的要合并的dev是在原有d、e版本上开发的（此刻dev已修复bug），这样合并会出错。 因而，需要先撤销G再合并，G为先前撤销合并恢复至主分支操作生成的编号。123git checkout mastergit revert Ggit merge dev 参考：https://blog.csdn.net/sndamhming/article/details/56011986","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"caffe-yolo summary","date":"2019-01-08T03:02:41.000Z","path":"2019/01/08/caffe-yolo-summary/","text":"本博文记录博主对caffe的初步理解以及yolo在caffe上的运行 一、数据处理篇1.1 Dataset转化为LMDB&nbsp;&nbsp;&nbsp;&nbsp;如先前所做的总结，在这里再次强调一下，首先要将数据转化为LMDB或LEVELDB格式，再输入至caffe的数据输入层。而图片转化为LMDB格式时，其形状或维度含义为[heights, weights, channels] 。其代码（位于caffe/src/caffe/util/io.cpp）如下:123456789101112131415161718192021222324void CVMatToDatum(const cv::Mat&amp; cv_img, Datum* datum) &#123;a CHECK(cv_img.depth() == CV_8U) &lt;&lt; \"Image data type must be unsigned byte\"; datum-&gt;set_channels(cv_img.channels()); datum-&gt;set_height(cv_img.rows); datum-&gt;set_width(cv_img.cols); datum-&gt;clear_data(); datum-&gt;clear_float_data(); datum-&gt;set_encoded(false); int datum_channels = datum-&gt;channels(); int datum_height = datum-&gt;height(); int datum_width = datum-&gt;width(); int datum_size = datum_channels * datum_height * datum_width; std::string buffer(datum_size, ' '); for (int h = 0; h &lt; datum_height; ++h) &#123; const uchar* ptr = cv_img.ptr&lt;uchar&gt;(h); int img_index = 0; for (int w = 0; w &lt; datum_width; ++w) &#123; for (int c = 0; c &lt; datum_channels; ++c) &#123; int datum_index = (c * datum_height + h) * datum_width + w; buffer[datum_index] = static_cast&lt;char&gt;(ptr[img_index++]); &#125; &#125; &#125; datum-&gt;set_data(buffer);&#125; 而label文件对bounding-box的标记也从[Xmin, Ymin, Xmax, Ymax] 转化为[Xmid, Ymid, W, H]，同时，对其进行了归一化操作；并将不同class转为对应的index（按照label_map进行`映射）。其代码（位于caffe/src/caffe/util/io.cpp）如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263void ParseXmlToDatum(const string&amp; annoname, const map&lt;string, int&gt;&amp; label_map, int ori_w, int ori_h, Datum* datum) &#123; ptree pt; read_xml(annoname, pt); int width(0), height(0); try &#123; height = pt.get&lt;int&gt;(\"annotation.size.height\"); width = pt.get&lt;int&gt;(\"annotation.size.width\"); CHECK_EQ(ori_w, width); CHECK_EQ(ori_h, height); &#125; catch (const ptree_error &amp;e) &#123; LOG(WARNING) &lt;&lt; \"When paring \" &lt;&lt; annoname &lt;&lt; \": \" &lt;&lt; e.what(); &#125; datum-&gt;clear_float_data(); BOOST_FOREACH(ptree::value_type &amp;v1, pt.get_child(\"annotation\")) &#123; if (v1.first == \"object\") &#123; ptree object = v1.second; int label(-1); vector&lt;float&gt; box(4, 0); int difficult(0); BOOST_FOREACH(ptree::value_type &amp;v2, object.get_child(\"\")) &#123; ptree pt2 = v2.second; if (v2.first == \"name\") &#123; string name = pt2.data(); // map name to label label = name_to_label(name, label_map); if (label &lt; 0) &#123; LOG(FATAL) &lt;&lt; \"Anno file \" &lt;&lt; annoname &lt;&lt; \" -&gt; unknown name: \" &lt;&lt; name; &#125; &#125; else if (v2.first == \"bndbox\") &#123; int xmin = pt2.get(\"xmin\", 0); int ymin = pt2.get(\"ymin\", 0); int xmax = pt2.get(\"xmax\", 0); int ymax = pt2.get(\"ymax\", 0); LOG_IF(WARNING, xmin &lt; 0 || xmin &gt; ori_w) &lt;&lt; annoname &lt;&lt; \" bounding box exceeds image boundary\"; LOG_IF(WARNING, xmax &lt; 0 || xmax &gt; ori_w) &lt;&lt; annoname &lt;&lt; \" bounding box exceeds image boundary\"; LOG_IF(WARNING, ymin &lt; 0 || ymin &gt; ori_h) &lt;&lt; annoname &lt;&lt; \" bounding box exceeds image boundary\"; LOG_IF(WARNING, ymax &lt; 0 || ymax &gt; ori_h) &lt;&lt; annoname &lt;&lt; \" bounding box exceeds image boundary\"; LOG_IF(WARNING, xmin &gt; xmax) &lt;&lt; annoname &lt;&lt; \" bounding box exceeds image boundary\"; LOG_IF(WARNING, ymin &gt; ymax) &lt;&lt; annoname &lt;&lt; \" bounding box exceeds image boundary\"; box[0] = float(xmin + (xmax - xmin) / 2.) / ori_w; box[1] = float(ymin + (ymax - ymin) / 2.) / ori_h; box[2] = float(xmax - xmin) / ori_w; box[3] = float(ymax - ymin) / ori_h; &#125; else if (v2.first == \"difficult\") &#123; difficult = atoi(pt2.data().c_str()); &#125; &#125; CHECK_GE(label, 0) &lt;&lt; \"label must start at 0\"; datum-&gt;add_float_data(float(label)); datum-&gt;add_float_data(float(difficult)); for (int i = 0; i &lt; 4; ++i) &#123; datum-&gt;add_float_data(box[i]); &#125; &#125; &#125;&#125; 1.2 DataLayeryolo网络训练、测试时所用的DataLayer是BoxDataLayer，该数据输入层是由caffe-yolo原作者编写。这里做一下简单的代码分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185#ifdef USE_OPENCV#include &lt;opencv2/core/core.hpp&gt;#endif // USE_OPENCV#include &lt;stdint.h&gt;#include &lt;vector&gt;#include \"caffe/data_transformer.hpp\"#include \"caffe/layers/box_data_layer.hpp\"#include \"caffe/util/benchmark.hpp\"namespace caffe &#123;//构造函数，初始化Layer参数，reader_参数; BasePrefetchingDataLayer带预取功能的数据读取层template &lt;typename Dtype&gt;BoxDataLayer&lt;Dtype&gt;::BoxDataLayer(const LayerParameter&amp; param) : BasePrefetchingDataLayer&lt;Dtype&gt;(param), reader_(param) &#123;&#125;//解析函数template &lt;typename Dtype&gt;BoxDataLayer&lt;Dtype&gt;::~BoxDataLayer() &#123; this-&gt;StopInternalThread();&#125;//BoxDataLayer层设置template &lt;typename Dtype&gt;void BoxDataLayer&lt;Dtype&gt;::DataLayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; this-&gt;box_label_ = true; const DataParameter param = this-&gt;layer_param_.data_param(); const int batch_size = param.batch_size(); // 读取数据，并使用它来初始化blob的top。 Datum&amp; datum = *(reader_.full().peek()); // 使用data_transformer从datum得到预期的blob形状。 vector&lt;int&gt; top_shape = this-&gt;data_transformer_-&gt;InferBlobShape(datum); this-&gt;transformed_data_.Reshape(top_shape); // Reshape top[0] and prefetch_data according to the batch_size. top_shape[0] = batch_size; top[0]-&gt;Reshape(top_shape); //PREFETCH_COUNT-预取的数据批量数目 for (int i = 0; i &lt; this-&gt;PREFETCH_COUNT; ++i) &#123; this-&gt;prefetch_[i].data_.Reshape(top_shape); &#125; LOG(INFO) &lt;&lt; \"output data size: \" &lt;&lt; top[0]-&gt;num() &lt;&lt; \",\" &lt;&lt; top[0]-&gt;channels() &lt;&lt; \",\" &lt;&lt; top[0]-&gt;height() &lt;&lt; \",\" &lt;&lt; top[0]-&gt;width(); // label if (this-&gt;output_labels_) &#123; if (param.side_size() &gt; 0) &#123; for (int i = 0; i &lt; param.side_size(); ++i) &#123; sides_.push_back(param.side(i)); &#125; &#125; if (sides_.size() == 0) &#123; sides_.push_back(7); &#125; CHECK_EQ(sides_.size(), top.size() - 1) &lt;&lt; \"side num not equal to top size\"; for (int i = 0; i &lt; this-&gt;PREFETCH_COUNT; ++i) &#123; this-&gt;prefetch_[i].multi_label_.clear(); &#125; for (int i = 0; i &lt; sides_.size(); ++i) &#123; vector&lt;int&gt; label_shape(1, batch_size); int label_size = sides_[i] * sides_[i] * (1 + 1 + 1 + 4); label_shape.push_back(label_size); top[i+1]-&gt;Reshape(label_shape); for (int j = 0; j &lt; this-&gt;PREFETCH_COUNT; ++j) &#123; shared_ptr&lt;Blob&lt;Dtype&gt; &gt; tmp_blob; tmp_blob.reset(new Blob&lt;Dtype&gt;(label_shape)); this-&gt;prefetch_[j].multi_label_.push_back(tmp_blob); &#125; &#125; &#125;&#125;// This function is called on prefetch thread// 批量导入数据template&lt;typename Dtype&gt;void BoxDataLayer&lt;Dtype&gt;::load_batch(Batch&lt;Dtype&gt;* batch) &#123; CPUTimer batch_timer; batch_timer.Start(); double read_time = 0; double trans_time = 0; CPUTimer timer; CHECK(batch-&gt;data_.count()); CHECK(this-&gt;transformed_data_.count()); // Reshape according to the first datum of each batch // on single input batches allows for inputs of varying dimension. const int batch_size = this-&gt;layer_param_.data_param().batch_size(); Datum&amp; datum = *(reader_.full().peek()); // Use data_transformer to infer the expected blob shape from datum. vector&lt;int&gt; top_shape = this-&gt;data_transformer_-&gt;InferBlobShape(datum); this-&gt;transformed_data_.Reshape(top_shape); // Reshape batch according to the batch_size. top_shape[0] = batch_size; batch-&gt;data_.Reshape(top_shape); Dtype* top_data = batch-&gt;data_.mutable_cpu_data(); vector&lt;Dtype*&gt; top_label; if (this-&gt;output_labels_) &#123; for (int i = 0; i &lt; sides_.size(); ++i) &#123; top_label.push_back(batch-&gt;multi_label_[i]-&gt;mutable_cpu_data()); &#125; &#125; for (int item_id = 0; item_id &lt; batch_size; ++item_id) &#123; timer.Start(); // get a datum Datum&amp; datum = *(reader_.full().pop(\"Waiting for data\")); read_time += timer.MicroSeconds(); timer.Start(); // Apply data transformations (mirror, scale, crop...) int offset = batch-&gt;data_.offset(item_id); vector&lt;BoxLabel&gt; box_labels; this-&gt;transformed_data_.set_cpu_data(top_data + offset); if (this-&gt;output_labels_) &#123; // rand sample a patch, adjust box labels this-&gt;data_transformer_-&gt;Transform(datum, &amp;(this-&gt;transformed_data_), &amp;box_labels); // transform label for (int i = 0; i &lt; sides_.size(); ++i) &#123; int label_offset = batch-&gt;multi_label_[i]-&gt;offset(item_id); int count = batch-&gt;multi_label_[i]-&gt;count(1); transform_label(count, top_label[i] + label_offset, box_labels, sides_[i]); &#125; &#125; else &#123; this-&gt;data_transformer_-&gt;Transform(datum, &amp;(this-&gt;transformed_data_)); &#125; trans_time += timer.MicroSeconds(); reader_.free().push(const_cast&lt;Datum*&gt;(&amp;datum)); &#125; timer.Stop(); batch_timer.Stop(); DLOG(INFO) &lt;&lt; \"Prefetch batch: \" &lt;&lt; batch_timer.MilliSeconds() &lt;&lt; \" ms.\"; DLOG(INFO) &lt;&lt; \" Read time: \" &lt;&lt; read_time / 1000 &lt;&lt; \" ms.\"; DLOG(INFO) &lt;&lt; \"Transform time: \" &lt;&lt; trans_time / 1000 &lt;&lt; \" ms.\";&#125;//生成通过数据转化器生成的数据对应的labeltemplate&lt;typename Dtype&gt;void BoxDataLayer&lt;Dtype&gt;::transform_label(int count, Dtype* top_label, const vector&lt;BoxLabel&gt;&amp; box_labels, int side) &#123; int locations = pow(side, 2); CHECK_EQ(count, locations * 7) &lt;&lt; \"side and count not match\"; // difficult caffe_set(locations, Dtype(0), top_label); // isobj caffe_set(locations, Dtype(0), top_label + locations); // class label caffe_set(locations, Dtype(-1), top_label + locations * 2); // box caffe_set(locations*4, Dtype(0), top_label + locations * 3); for (int i = 0; i &lt; box_labels.size(); ++i) &#123; float difficult = box_labels[i].difficult_; if (difficult != 0. &amp;&amp; difficult != 1.) &#123; LOG(WARNING) &lt;&lt; \"Difficult must be 0 or 1\"; &#125; float class_label = box_labels[i].class_label_; CHECK_GE(class_label, 0) &lt;&lt; \"class_label must &gt;= 0\"; float x = box_labels[i].box_[0]; float y = box_labels[i].box_[1]; // LOG(INFO) &lt;&lt; \"x: \" &lt;&lt; x &lt;&lt; \" y: \" &lt;&lt; y; int x_index = floor(x * side); int y_index = floor(y * side); x_index = std::min(x_index, side - 1); y_index = std::min(y_index, side - 1); int dif_index = side * y_index + x_index; int obj_index = locations + dif_index; int class_index = locations * 2 + dif_index; int cor_index = locations * 3 + dif_index * 4; top_label[dif_index] = difficult; top_label[obj_index] = 1; // LOG(INFO) &lt;&lt; \"dif_index: \" &lt;&lt; dif_index &lt;&lt; \" class_label: \" &lt;&lt; class_label; top_label[class_index] = class_label; for (int j = 0; j &lt; 4; ++j) &#123; top_label[cor_index + j] = box_labels[i].box_[j]; &#125; &#125;&#125;//实例化BoxDataLayer、BoxDataINSTANTIATE_CLASS(BoxDataLayer);REGISTER_LAYER_CLASS(BoxData);&#125; // namespace caffe 1.3 Input与预处理&nbsp;&nbsp;&nbsp;&nbsp;在进行图像预处理时，可以使用去均值操作，其目的是使得像素值更接近（0,0,0）原点，从而加快收敛速度。如果在数据层加入去均值操作，预测时也需要进行去均值操作。如无，则无需！其方法如下：1234//(104,117,123)为imagenet均值，可自行根据数据集生成均值。mean_value: 104mean_value: 117mean_value: 123 &nbsp;&nbsp;&nbsp;&nbsp;同时，图像预处理的目的之一是保证输入数据与网络输入层所要求的shape保持一致。通过opencv.imread(img_path)函数读取的图片为（heights, weights, channels）。而deploy.prototxt中的input层为（channels, heights, weights）。因此，在进行预测时需要对输入图片进行预处理。12345im = cv2.imread(im_path)im = cv2.resize(im, (160, 160))im = np.require(im.transpose((2, 0, 1)), dtype=np.float32)#在训练时没有进行去均值，因此在预测时也没有进行去均值。#im -= mean 而转化为graph文件后，其网络输入层为（heights, weights, channels），附件为转为graph文件后的网络结构图：点击查看或下载因此，无需对输入图片进行预处理：12im = cv2.imread(input_image_path)im = cv2.resize(im, (160, 160)) 二、caffe简介2.1 Project结构&nbsp;&nbsp;&nbsp;&nbsp;在caffe架构下搭建网络是通过prototxt文件描述的，以此建立统一的参数管理机制。在prototxt文件中，不仅包含基本的网络结构，还包含Loss层（Train时需要）、输入数据的路径和结构（Train与Test时需要）、输入数据size/ shape（如1601603,Deploy时需要）。因此，不同于keras，caffe的网络结构文件需要多个。&nbsp;&nbsp;&nbsp;&nbsp;首先，solver.prototxt（即求解器）的主要功能是设置超参数，确定优化方式；其次， train.prototxt与test.prototxt的主要功能是搭建网络结构，设置结构参数用于训练与测试，确定loss层；最后，deploy.prototxt的主要功能是搭建最基础的网络结构用于预测。 2.2 网络结构deploy.prototxt内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396name: \"tiny-yolo\"input: \"data\"input_shape &#123; dim: 1 dim: 3 dim: 160 dim: 160&#125;layer &#123; name: \"conv1\" type: \"Convolution\" bottom: \"data\" top: \"conv1\" convolution_param &#123; num_output: 16 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn1\" type: \"BatchNorm\" bottom: \"conv1\" top: \"bn1\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale1\" type: \"Scale\" bottom: \"bn1\" top: \"scale1\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu1\" type: \"ReLU\" bottom: \"scale1\" top: \"scale1\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"pool1\" type: \"Pooling\" bottom: \"scale1\" top: \"pool1\" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: \"conv2\" type: \"Convolution\" bottom: \"pool1\" top: \"conv2\" convolution_param &#123; num_output: 32 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn2\" type: \"BatchNorm\" bottom: \"conv2\" top: \"bn2\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale2\" type: \"Scale\" bottom: \"bn2\" top: \"scale2\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu2\" type: \"ReLU\" bottom: \"scale2\" top: \"scale2\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"pool2\" type: \"Pooling\" bottom: \"scale2\" top: \"pool2\" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: \"conv3\" type: \"Convolution\" bottom: \"pool2\" top: \"conv3\" convolution_param &#123; num_output: 64 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn3\" type: \"BatchNorm\" bottom: \"conv3\" top: \"bn3\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale3\" type: \"Scale\" bottom: \"bn3\" top: \"scale3\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu3\" type: \"ReLU\" bottom: \"scale3\" top: \"scale3\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"pool3\" type: \"Pooling\" bottom: \"scale3\" top: \"pool3\" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: \"conv4\" type: \"Convolution\" bottom: \"pool3\" top: \"conv4\" convolution_param &#123; num_output: 128 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn4\" type: \"BatchNorm\" bottom: \"conv4\" top: \"bn4\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale4\" type: \"Scale\" bottom: \"bn4\" top: \"scale4\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu4\" type: \"ReLU\" bottom: \"scale4\" top: \"scale4\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"pool4\" type: \"Pooling\" bottom: \"scale4\" top: \"pool4\" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: \"conv5\" type: \"Convolution\" bottom: \"pool4\" top: \"conv5\" convolution_param &#123; num_output: 256 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn5\" type: \"BatchNorm\" bottom: \"conv5\" top: \"bn5\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale5\" type: \"Scale\" bottom: \"bn5\" top: \"scale5\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu5\" type: \"ReLU\" bottom: \"scale5\" top: \"scale5\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"pool5\" type: \"Pooling\" bottom: \"scale5\" top: \"pool5\" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: \"conv6\" type: \"Convolution\" bottom: \"pool5\" top: \"conv6\" convolution_param &#123; num_output: 512 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn6\" type: \"BatchNorm\" bottom: \"conv6\" top: \"bn6\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale6\" type: \"Scale\" bottom: \"bn6\" top: \"scale6\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu6\" type: \"ReLU\" bottom: \"scale6\" top: \"scale6\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"pool6\" type: \"Pooling\" bottom: \"scale6\" top: \"pool6\" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: \"conv7\" type: \"Convolution\" bottom: \"pool6\" top: \"conv7\" convolution_param &#123; num_output: 1024 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn7\" type: \"BatchNorm\" bottom: \"conv7\" top: \"bn7\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale7\" type: \"Scale\" bottom: \"bn7\" top: \"scale7\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu7\" type: \"ReLU\" bottom: \"scale7\" top: \"scale7\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"conv8\" type: \"Convolution\" bottom: \"scale7\" top: \"conv8\" convolution_param &#123; num_output: 256 kernel_size: 3 pad: 1 bias_term: false &#125;&#125;layer &#123; name: \"bn8\" type: \"BatchNorm\" bottom: \"conv8\" top: \"bn8\" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; name: \"scale8\" type: \"Scale\" bottom: \"bn8\" top: \"scale8\" scale_param &#123; bias_term: true &#125;&#125;layer &#123; name: \"relu8\" type: \"ReLU\" bottom: \"scale8\" top: \"scale8\" relu_param &#123; negative_slope: 0.1 &#125;&#125;layer &#123; name: \"fc9\" type: \"InnerProduct\" bottom: \"scale8\" top: \"fc9\" inner_product_param &#123; num_output: 300 &#125;&#125; 三、Tiny YOLO&nbsp;&nbsp;&nbsp;&nbsp;Tiny-YOLO是YOLO算法的简单实现。相比于YOLO算法，它的网络结构更浅，仅有9层。除此外，其理论基础与YOLO并无二致。 3.1 Yolo Innovation&nbsp;&nbsp;&nbsp;&nbsp;YOLO算法首创的实现了端到端的目标检测算法，是速度惊人、准确度较好的one-stage算法。YOLO算法将整张图片划分为SXS的grid，采用一次性预测所有格子所含目标的bounding-box、confidence以及P(object)和P(class|object)。&nbsp;&nbsp;&nbsp;&nbsp;网络的输出结果为一个向量，size为：S S (B 5 +C)。其中，S为划分网格数，B为每个网格负责目标个数，C为类别个数。其含义为：每个网格会对应B个边界框，边界框的宽高范围为全图，而中心点落于该网格；每个边界框对应一个置信度值，代表该处是否有物体及定位准确度（即Confidence = P(object) IOU(predict-box, ground-truth)。）；每个网格对应C个概率，分别代表每个class出现的概率。&nbsp;&nbsp;&nbsp;&nbsp;而YOLO是如何实现对输入图像的分格呢？&nbsp;&nbsp;&nbsp;&nbsp;原作者巧妙地在最后预测层设置了S S (B 5 +C)个神经元（该层为全连接层，在yolo2中该层为11的卷积层），通过训练将对应不同网格的ground-truth收敛到对应的网格的输出中。 3.2 Loss&nbsp;&nbsp;&nbsp;&nbsp;损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。简单的全部采用了sum-squared error loss来做这件事会有以下不足：首先，(num_side*4)维的localization error和(num_classes)维的classification error每一个维度产生的代价同等重要，这显然是不合理的。其次，如果一些栅格中没有object（一幅图中这种栅格很多），那么就会将这些栅格中的bounding box的confidence置为0，相比于较少的有object的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。&nbsp;&nbsp;&nbsp;&nbsp;因此，YOLO采取了更有效的Loss函数。将loss函数分为3部分：第一，坐标预测是否准确(图片中书写有误，xy值与groundtruth应相减不因相加)；第二，有无object预测是否准确；第三，类别预测。&nbsp;&nbsp;&nbsp;&nbsp;更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为 λcoord ,在pascal VOC训练中取5。对没有object的bbox的confidence loss，赋予小的loss weight，记为 λnoobj ，在pascal VOC训练中取0.5。有object的bbox的confidence loss 和类别的loss 的loss weight正常取1。&nbsp;&nbsp;&nbsp;&nbsp;对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏相同的尺寸对IOU的影响更大。而sum-square error loss中对同样的偏移loss是一样。为了缓和这个问题，作者用了一个巧妙的办法，就是将box的width和height取平方根代替原本的height和width。 如下：small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。 四、Train &amp;&amp; Test4.1 Optimization本项目测试过SGD、momentum 、Adam。最终，Adam效果最佳。 4.2 solver.prototxt(Adam)123456789101112131415161718192021net: \"x_train.prototxt\"test_iter: 3000test_interval: 32000test_initialization: falsedisplay: 20average_loss: 100lr_policy: \"multifixed\"stagelr: 0.001stagelr: 0.0001stagelr: 0.00001stagelr: 0.000001stageiter: 520stageiter: 16000stageiter: 24000stageiter: 32000max_iter: 32000momentum: 0.9weight_decay: 0.0005snapshot: 2000snapshot_prefix: \"./models/x_yolo\"solver_mode: GPU 4.3 train.prototxt点击下载 五、Predict123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#!/usr/bin/env pythonimport numpy as npimport cv2import osimport syssys.path.insert(0, '/home/mc/Desktop/caffe/caffe/python/')import caffe############################global variable set start############################num_classes = 2num_anchors = 2side = 5net_proto = \"./x_deploy.prototxt\"model_path = \"./models/x_yolo_iter_32000.caffemodel\"im_path = '/home/mc/Desktop/caffe-yolo/data/yolo/VOCdevkit/VOC2018/JPEGImages/826.jpg'############################global variable set end..#############################environment setscaffe.set_device(0)caffe.set_mode_gpu()#nms filterdef nms(boxes, thresh): x1 = boxes[:, 0] - boxes[:, 2] / 2. y1 = boxes[:, 1] - boxes[:, 3] / 2. x2 = boxes[:, 0] + boxes[:, 2] / 2. y2 = boxes[:, 1] + boxes[:, 3] / 2. scores = boxes[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) order = scores.argsort()[::-1] keep = [] while order.size &gt; 0: i = order[0] keep.append(i) ix1 = np.maximum(x1[i], x1[order[1:]]) iy1 = np.maximum(y1[i], y1[order[1:]]) ix2 = np.minimum(x2[i], x2[order[1:]]) iy2 = np.minimum(y2[i], y2[order[1:]]) w = np.maximum(0.0, ix2-ix1+1) h = np.maximum(0.0, iy2-iy1+1) inter = w * h ovr = inter / (areas[i] + areas[order[1:]] - inter) inds = np.where(ovr &lt;= thresh)[0] order = order[inds + 1] return boxes[np.require(keep), :]#parse resultdef parse_result(out_put): global num_classes global num_anchors global side locations = side ** 2 boxes = np.zeros((num_anchors * locations, 6), dtype=np.float32) for i in range(locations): tmp_scores = out_put[i:num_classes*locations:locations] max_class_ind = np.argsort(tmp_scores)[-1] max_prob = np.max(tmp_scores) obj_index = num_classes * locations + i obj_scores = max_prob * out_put[obj_index:(obj_index+num_anchors*locations):locations] coor_index = (num_classes + num_anchors) * locations + i for j in range(num_anchors): boxes[i*num_anchors+j][5] = max_class_ind boxes[i*num_anchors+j][4] = obj_scores[j] box_index = coor_index + j * 4 * locations boxes[i*num_anchors+j][0] = (i % side + out_put[box_index + 0 * locations]) / float(side) boxes[i*num_anchors+j][1] = (i / side + out_put[box_index + 1 * locations]) / float(side) boxes[i*num_anchors+j][2] = out_put[box_index + 2 * locations] ** 2 boxes[i*num_anchors+j][3] = out_put[box_index + 3 * locations] ** 2 return nms(boxes, 0.5)#show or write result_picturedef show_boxes(im_path, boxes, sthresh=0.5, hthresh=1, show=0): print (boxes.shape) im = cv2.imread(im_path) ori_w = im.shape[1] ori_h = im.shape[0] for box in boxes: if box[4] &lt; sthresh: continue if box[4] &gt; hthresh: continue print (box) box = box[:4] x1 = max(0, int((box[0] - box[2] / 2.) * ori_w)) y1 = max(0, int((box[1] - box[3] / 2.) * ori_h)) x2 = min(ori_w - 1, int((box[0] + box[2] / 2.) * ori_w)) y2 = min(ori_h - 1, int((box[1] + box[3] / 2.) * ori_h)) cv2.rectangle(im, (x1, y1), (x2, y2), (0, 255, 255), 2) name = os.path.split(im_path)[1].split('.')[0] if show: cv2.imshow(\"out\", im) else: cv2.imwrite(\"adam-out-n\"+name+'.jpg', im)# predictdef predict(model, im_path): # image pre-processing im = cv2.imread(im_path) im = cv2.resize(im, (160, 160)) im = np.require(im.transpose((2, 0, 1)), dtype=np.float32) # forward process... model.blobs['data'].data[...] = im out_blobs = model.forward() ''' The structure of out_put is: [n*n*class1,n*n*class2,...,n*n*class(i),n*n*score1,n*n*score2,n*n*(x,y,w,h)1,n*n*(x,y,w,h)2] p.s. n is side ''' reg_out = out_blobs[\"fc9\"] boxes = parse_result(reg_out[0]) show_boxes(im_path, boxes, 0.2)if __name__==\"__main__\": global net_proto global model_path global im_path # load net with model model = caffe.Net(net_proto, model_path, caffe.TEST) predict(model, im_path)","tags":[{"name":"caffe","slug":"caffe","permalink":"http://yoursite.com/tags/caffe/"},{"name":"cv","slug":"cv","permalink":"http://yoursite.com/tags/cv/"}]},{"title":"安装教程：Ubuntu16.04+CUDA9.0+CUDNN7.1+caffe(gpu)+opencv","date":"2018-12-27T17:45:17.000Z","path":"2018/12/28/安装教程：Ubuntu16-04-CUDA9-0-CUDNN7-1-caffe-gpu-opencv/","text":"本教程适用于CUDA9.0+CUDNN7.1+OPENCV(3.4.0) 一、依赖包安装 在Ubuntu的Terminal中输入：1234sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libopenblas-dev liblapack-dev libatlas-base-devsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev 二、驱动安装(Nvidia 384.X版本的驱动)方式一在Terminal输入：123456sudo apt-get update sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-384 sudo apt-get install mesa-common-dev sudo apt-getinstall freeglut3-dev 方式二直接在Ubuntu中的System Settings–&gt;Software&amp;Updates中的additional drivers：驱动安装成功的标志： 三、CUDA9.0安装请通过官网下载CUDA安装文件（.run文件)，运行文件命令如下：12# 先CD至.run文件的文件夹，再运行该命令sudo sh cuda_9.0.176_384.81_linux.run 先按q直接跳过阅读协议，然后accept，后面的除了Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81?这样的选n,其它的有y选y，或者直接回车默认检查一下环境变量1gedit ~/.bashr 末尾添加123#cudaexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64/:$LD_LIBRARY_PATHexport PATH=/usr/local/cuda-9.0/bin:$PATH 然后激活1source ~/.bashrc 检验安装是否完整： 四、CUDNN7.1安装请通过官网下载CUDNN安装文件（.tgz文件)，直接在Terminal中cd至所在文件夹，运行以下命令：12345tar -zxvf cudnn-9.0-linux-x64-v7.1.tgz sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -d sudo chmod a+r /usr/local/cuda/include/cudnn.h sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 检验是否安装完整： 五、Opencv源码编译安装将下载好的opencv源码（.zip文件）解压缩至home文件夹下，然后在Terminal中输入：123456cd ~/opencv-3.4.1mkdir build cd build cmake -D CMAKE_BUILD_TYPE=Release .. sudo make -j8sudo make install 安装完后检验: 六、Caffe安装此处我直接安装到home目录，执行：12cd ~ git clone https://github.com/BVLC/caffe.git #开始clone 等待下载结束，下载结束后在你的home路径下会存在，caffe文件夹。接下来进入caffe并开始配置caffe，配置如下:12sudo cp Makefile.config.example Makefile.configsudo gedit Makefile.config #或者sudo vim Makefile.config 修改Makefile.config内容：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849将：#USE_CUDNN := 1修改为： USE_CUDNN := 1将：#OPENCV_VERSION := 3 修改为： OPENCV_VERSION := 3将：#WITH_PYTHON_LAYER := 1修改为WITH_PYTHON_LAYER := 1将：INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib修改为：INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial将# CUDA architecture setting: going with all of them.# For CUDA &amp;lt; 6.0, comment the *_50 through *_61 lines for compatibility.# For CUDA &amp;lt; 8.0, comment the *_60 and *_61 lines for compatibility.# For CUDA &amp;gt;= 9.0, comment the *_20 and *_21 lines for compatibility.CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\ -gencode arch=compute_20,code=sm_21 \\ -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_60,code=sm_60 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61改为：# CUDA architecture setting: going with all of them.# For CUDA &amp;lt; 6.0, comment the *_50 through *_61 lines for compatibility.# For CUDA &amp;lt; 8.0, comment the *_60 and *_61 lines for compatibility.# For CUDA &amp;gt;= 9.0, comment the *_20 and *_21 lines for compatibility.CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_60,code=sm_60 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61 修改Makefile文件：123456789将：NVCCFLAGS +=-ccbin=$(CXX) -Xcompiler-fPIC $(COMMON_FLAGS)替换为：NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)将：LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5改为：LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_serial_hl hdf5_serial 配置完好之后开始编译：123456cd caffesudo make cleansudo make all #或者make all -j4(代表4核，或者j8)sudo make testsudo make runtest #或者sudo make runtest -j8sudo make pycaffe 检验是否安装完整：所有的test中，如果编译不报错，则说明安装完整。","tags":[{"name":"caffe","slug":"caffe","permalink":"http://yoursite.com/tags/caffe/"}]}]